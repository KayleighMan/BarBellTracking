{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv8 model Training for Barbell Detection and Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.182 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.91  Python-3.9.21 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i5-13600KF)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:\\Users\\kayma\\Downloads\\Bar_Path_Tracking.v4i.yolov8\\data.yaml, epochs=20, time=None, patience=100, batch=4, imgsz=416, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train5', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\kayma\\Downloads\\Bar_Path_Tracking.v4i.yolov8\\train\\labels.cache... 1606 images, 6 backgrounds, 0 corrupt: 100%|██████████| 1606/1606 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kayma\\Downloads\\Bar_Path_Tracking.v4i.yolov8\\valid\\labels.cache... 151 images, 0 backgrounds, 0 corrupt: 100%|██████████| 151/151 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module 'matplotlib' has no attribute 'backends'\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train5\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20         0G      2.245      9.476      1.342          8        416:   0%|          | 1/402 [00:01<09:16,  1.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolov8s.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mkayma\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mBar_Path_Tracking.v4i.yolov8\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m416\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kayma\\anaconda3\\envs\\yolov8_env\\lib\\site-packages\\ultralytics\\engine\\model.py:791\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\kayma\\anaconda3\\envs\\yolov8_env\\lib\\site-packages\\ultralytics\\engine\\trainer.py:211\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kayma\\anaconda3\\envs\\yolov8_env\\lib\\site-packages\\ultralytics\\engine\\trainer.py:396\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n\u001b[1;32m--> 396\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m     last_opt_step \u001b[38;5;241m=\u001b[39m ni\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;66;03m# Timed stopping\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kayma\\anaconda3\\envs\\yolov8_env\\lib\\site-packages\\ultralytics\\engine\\trainer.py:610\u001b[0m, in \u001b[0;36mBaseTrainer.optimizer_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39munscale_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)  \u001b[38;5;66;03m# unscale gradients\u001b[39;00m\n\u001b[0;32m    609\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m)  \u001b[38;5;66;03m# clip gradients\u001b[39;00m\n\u001b[1;32m--> 610\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\kayma\\anaconda3\\envs\\yolov8_env\\lib\\site-packages\\torch\\amp\\grad_scaler.py:380\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke ``unscale_(optimizer)`` followed by parameter update, if gradients are not infs/NaN.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m:meth:`step` carries out the following two operations:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03m    Closure use is not currently supported.\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enabled:\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosure\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosure use is not currently supported if GradScaler is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\kayma\\anaconda3\\envs\\yolov8_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kayma\\anaconda3\\envs\\yolov8_env\\lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kayma\\anaconda3\\envs\\yolov8_env\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\kayma\\anaconda3\\envs\\yolov8_env\\lib\\site-packages\\torch\\optim\\adamw.py:243\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    230\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    232\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    233\u001b[0m         group,\n\u001b[0;32m    234\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m         state_steps,\n\u001b[0;32m    241\u001b[0m     )\n\u001b[1;32m--> 243\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\kayma\\anaconda3\\envs\\yolov8_env\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kayma\\anaconda3\\envs\\yolov8_env\\lib\\site-packages\\torch\\optim\\adamw.py:875\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 875\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kayma\\anaconda3\\envs\\yolov8_env\\lib\\site-packages\\torch\\optim\\adamw.py:477\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    475\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 477\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    479\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8s.pt\")  \n",
    "\n",
    "# Train the model\n",
    "model.train(\n",
    "    data=\"C:\\\\Users\\\\kayma\\\\Downloads\\\\Bar_Path_Tracking.v4i.yolov8\\\\data.yaml\",\n",
    "    epochs=20, \n",
    "    imgsz=416, \n",
    "    batch=4, \n",
    "    device=\"cpu\"  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MoveNet for pose detection, and YOLOv8 for barbell tracking and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kayma\\anaconda3\\envs\\yolov8_env\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video: Output_Video\\Blue_Plates_BB_Squat_Dark Setting_output_movenet.mp4\n",
      "Average FPS: 13.26\n",
      "Per-frame CSV: Output_Video\\Results\\Blue_Plates_BB_Squat_Dark Setting_results_movenet_perframe.csv\n",
      "Summary CSV: Output_Video\\Results\\Blue_Plates_BB_Squat_Dark Setting_results_movenet_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =============================\n",
    "# 1) Load models\n",
    "# =============================\n",
    "# Barbell detector\n",
    "yolo_model = YOLO(\"runs/detect/train3/weights/best.pt\")\n",
    "\n",
    "# MoveNet Thunder TFLite\n",
    "interpreter = tf.lite.Interpreter(model_path=\"movenet_thunder.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# =============================\n",
    "# 2) Globals & thresholds\n",
    "# =============================\n",
    "barbell_path = []\n",
    "bar_center_history = []\n",
    "sway_values = []\n",
    "log_data = []\n",
    "\n",
    "rep_count = 0\n",
    "squat_down = False\n",
    "feedback_message = \"\"\n",
    "\n",
    "squat_down_angle = 90   # descend threshold\n",
    "squat_up_angle   = 160  # ascend threshold\n",
    "max_sway = 50           # px sway window\n",
    "\n",
    "# =============================\n",
    "# 3) Helpers\n",
    "# =============================\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"Angle at b in degrees.\"\"\"\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    return 360 - angle if angle > 180 else int(angle)\n",
    "\n",
    "# =============================\n",
    "# 4) Pose estimation + squat logic\n",
    "# =============================\n",
    "def detect_pose(frame, frame_idx):\n",
    "    global rep_count, squat_down, feedback_message\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    inp = cv2.resize(frame, (256, 256))\n",
    "    inp = cv2.cvtColor(inp, cv2.COLOR_BGR2RGB)\n",
    "    inp = np.expand_dims(inp, axis=0).astype(np.uint8)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], inp)\n",
    "    interpreter.invoke()\n",
    "    keypoints = interpreter.get_tensor(output_details[0]['index'])[0][0]\n",
    "\n",
    "    # Skeleton\n",
    "    skeleton = [\n",
    "        (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n",
    "        (5, 11), (6, 12), (11, 12), (11, 13), (13, 15),\n",
    "        (12, 14), (14, 16)\n",
    "    ]\n",
    "\n",
    "    points = []\n",
    "    for y, x, conf in keypoints:\n",
    "        if conf > 0.3:\n",
    "            cx, cy = int(x * w), int(y * h)\n",
    "            points.append((cx, cy))\n",
    "            cv2.circle(frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "        else:\n",
    "            points.append(None)\n",
    "\n",
    "    for a, b in skeleton:\n",
    "        if points[a] and points[b]:\n",
    "            cv2.line(frame, points[a], points[b], (0, 255, 255), 2)\n",
    "\n",
    "    avg_knee_angle = None\n",
    "    try:\n",
    "        l_hip, l_knee, l_ank = points[11], points[13], points[15]\n",
    "        r_hip, r_knee, r_ank = points[12], points[14], points[16]\n",
    "        angles = []\n",
    "\n",
    "        if l_hip and l_knee and l_ank:\n",
    "            angles.append(calculate_angle(l_hip, l_knee, l_ank))\n",
    "        if r_hip and r_knee and r_ank:\n",
    "            angles.append(calculate_angle(r_hip, r_knee, r_ank))\n",
    "\n",
    "        if angles:\n",
    "            avg_knee_angle = int(np.mean(angles))\n",
    "\n",
    "            # rep logic\n",
    "            if avg_knee_angle < squat_down_angle and not squat_down:\n",
    "                squat_down = True\n",
    "            if avg_knee_angle > squat_up_angle and squat_down:\n",
    "                rep_count += 1\n",
    "                squat_down = False\n",
    "\n",
    "            # feedback\n",
    "            if avg_knee_angle > squat_up_angle:\n",
    "                feedback_message = \"Too Upright\"\n",
    "            elif avg_knee_angle < 60:\n",
    "                feedback_message = \"Too Deep!\"\n",
    "            else:\n",
    "                feedback_message = \"Good Depth\"\n",
    "\n",
    "            # overlay angle\n",
    "            cv2.putText(frame, f\"Knee Angle: {avg_knee_angle} degrees\", (30, 80),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    log_data.append({\n",
    "        \"frame\": frame_idx,\n",
    "        \"angle\": avg_knee_angle,\n",
    "        \"reps\": rep_count,\n",
    "        \"feedback\": feedback_message,\n",
    "        \"sway\": None\n",
    "    })\n",
    "\n",
    "    return frame\n",
    "\n",
    "# =============================\n",
    "# 5) Barbell tracking\n",
    "# =============================\n",
    "def track_barbell(frame, results, frame_idx):\n",
    "    global feedback_message\n",
    "\n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        conf = float(box.conf[0])\n",
    "        if conf > 0.4:\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            barbell_path.append((cx, cy))\n",
    "            bar_center_history.append(cx)\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "            cv2.putText(frame, \"Barbell\", (x1, y1 - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "    for i in range(1, len(barbell_path)):\n",
    "        cv2.line(frame, barbell_path[i-1], barbell_path[i], (0, 165, 255), 2)\n",
    "\n",
    "    if len(bar_center_history) > 10:\n",
    "        sway_range = max(bar_center_history[-10:]) - min(bar_center_history[-10:])\n",
    "        sway_values.append(sway_range)\n",
    "        if sway_range > max_sway:\n",
    "            feedback_message = \"Bar Sway Detected!\"\n",
    "\n",
    "        if len(log_data) > 0 and log_data[-1][\"frame\"] == frame_idx:\n",
    "            log_data[-1][\"sway\"] = sway_range\n",
    "\n",
    "    return frame\n",
    "\n",
    "# =============================\n",
    "# 6) Video I/O\n",
    "# =============================\n",
    "video_folder = \"Input_Video\"\n",
    "video_filename = \"Blue_Plates_BB_Squat_Dark Setting.mp4\"\n",
    "video_path = os.path.join(video_folder, video_filename)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Could not open video: {video_path}\")\n",
    "\n",
    "output_folder = \"Output_Video\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "results_folder = os.path.join(output_folder, \"Results\")\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "output_filename = \"Blue_Plates_BB_Squat_Dark Setting_output_movenet.mp4\"\n",
    "output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps_in = cap.get(cv2.CAP_PROP_FPS)\n",
    "w = int(cap.get(3)); h = int(cap.get(4))\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps_in, (w, h))\n",
    "\n",
    "# =============================\n",
    "# 7) Main loop\n",
    "# =============================\n",
    "frame_idx = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    pose_frame = detect_pose(frame.copy(), frame_idx)\n",
    "\n",
    "    results = yolo_model(frame, imgsz=416, conf=0.35, verbose=False)[0]\n",
    "    pose_frame = track_barbell(pose_frame, results, frame_idx)\n",
    "\n",
    "    cv2.putText(pose_frame, f\"Reps: {rep_count}\", (30, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    if feedback_message:\n",
    "        cv2.putText(pose_frame, feedback_message, (30, 120),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 3)\n",
    "\n",
    "    cv2.imshow(\"Squat Analyzer (MoveNet)\", pose_frame)\n",
    "    out.write(pose_frame)\n",
    "\n",
    "    frame_idx += 1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "avg_fps = frame_idx / elapsed if elapsed > 0 else 0.0\n",
    "print(f\"Saved video: {output_path}\")\n",
    "print(f\"Average FPS: {avg_fps:.2f}\")\n",
    "\n",
    "# =============================\n",
    "# 8) Save logs\n",
    "# =============================\n",
    "df = pd.DataFrame(log_data)\n",
    "df[\"fps\"] = avg_fps\n",
    "perframe_csv = os.path.join(results_folder, \"Blue_Plates_BB_Squat_Dark Setting_results_movenet_perframe.csv\")\n",
    "df.to_csv(perframe_csv, index=False)\n",
    "print(\"Per-frame CSV:\", perframe_csv)\n",
    "\n",
    "summary = {\n",
    "    \"total_frames\": frame_idx,\n",
    "    \"total_reps\": rep_count,\n",
    "    \"avg_fps\": avg_fps,\n",
    "    \"mean_angle\": float(df[\"angle\"].dropna().mean()) if df[\"angle\"].notna().any() else None,\n",
    "    \"std_angle\": float(df[\"angle\"].dropna().std()) if df[\"angle\"].notna().any() else None,\n",
    "    \"mean_sway\": float(df[\"sway\"].dropna().mean()) if df[\"sway\"].notna().any() else None,\n",
    "    \"max_sway\": float(df[\"sway\"].dropna().max()) if df[\"sway\"].notna().any() else None\n",
    "}\n",
    "summary_csv = os.path.join(results_folder, \"Blue_Plates_BB_Squat_Dark Setting_results_movenet_summary.csv\")\n",
    "pd.DataFrame([summary]).to_csv(summary_csv, index=False)\n",
    "print(\"Summary CSV:\", summary_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using YOLOPose for pose detection, and YOLOv8 for barbell tracking and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video: Output_Video\\Blue_Plates_BB_Squat_Dark Setting_output_pose_squat_system.mp4\n",
      "Average FPS: 12.18\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =============================\n",
    "# 1) Load models\n",
    "# =============================\n",
    "# Your barbell detector\n",
    "yolo_model = YOLO(\"runs/detect/train3/weights/best.pt\")\n",
    "\n",
    "# Pose model (pretrained COCO-17)\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\") \n",
    "\n",
    "# =============================\n",
    "# 2) Globals & thresholds\n",
    "# =============================\n",
    "barbell_path = []\n",
    "bar_center_history = []\n",
    "sway_values = []\n",
    "\n",
    "rep_count = 0\n",
    "squat_down = False\n",
    "feedback_message = \"\"\n",
    "\n",
    "# per-frame log\n",
    "log_data = []\n",
    "\n",
    "# thresholds\n",
    "squat_down_angle = 90     # start counting when below this\n",
    "squat_up_angle   = 160    # finish rep when above this\n",
    "max_sway = 50             # px (horizontal) over last 10 frames\n",
    "\n",
    "# =============================\n",
    "# 3) Helpers\n",
    "# =============================\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"Angle at b (in degrees) formed by points a-b-c.\"\"\"\n",
    "    a = np.array(a); b = np.array(b); c = np.array(c)\n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    return 360 - angle if angle > 180 else int(angle)\n",
    "\n",
    "# =============================\n",
    "# 4) Pose estimation (YOLOv8-Pose)\n",
    "# =============================\n",
    "def detect_pose(frame, frame_idx):\n",
    "    \"\"\"\n",
    "    Runs YOLOv8-Pose; draws full COCO skeleton; computes knee angle,\n",
    "    updates rep counter + feedback; logs per-frame metrics.\n",
    "    Returns (annotated_frame, avg_knee_angle or None)\n",
    "    \"\"\"\n",
    "    global rep_count, squat_down, feedback_message\n",
    "\n",
    "    res = pose_model(frame, imgsz=416, conf=0.25, verbose=False)\n",
    "    if len(res) == 0:\n",
    "        # log empty row to keep indexing consistent\n",
    "        log_data.append({\n",
    "            \"frame\": frame_idx, \"angle\": None, \"reps\": rep_count,\n",
    "            \"feedback\": feedback_message, \"sway\": None\n",
    "        })\n",
    "        return frame, None\n",
    "\n",
    "    r0 = res[0]\n",
    "    if r0.keypoints is None or r0.keypoints.xy is None or len(r0.keypoints.xy) == 0:\n",
    "        log_data.append({\n",
    "            \"frame\": frame_idx, \"angle\": None, \"reps\": rep_count,\n",
    "            \"feedback\": feedback_message, \"sway\": None\n",
    "        })\n",
    "        return frame, None\n",
    "\n",
    "    # use first person (optionally: pick largest bbox)\n",
    "    kxy = r0.keypoints.xy[0]\n",
    "    try:\n",
    "        kxy = kxy.cpu().numpy()\n",
    "    except Exception:\n",
    "        kxy = np.array(kxy)\n",
    "\n",
    "    # try to get confidences if available\n",
    "    try:\n",
    "        kdata = r0.keypoints.data[0].cpu().numpy()  # (K,3) -> [x,y,conf]\n",
    "        kconf = kdata[:, 2]\n",
    "    except Exception:\n",
    "        kconf = np.ones((kxy.shape[0],), dtype=float)\n",
    "\n",
    "    # COCO-17 skeleton pairs (same as your MoveNet skeleton subset)\n",
    "    skeleton = [\n",
    "        (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),\n",
    "        (5, 11), (6, 12), (11, 12), (11, 13), (13, 15),\n",
    "        (12, 14), (14, 16)\n",
    "    ]\n",
    "    vis_thr = 0.3\n",
    "\n",
    "    # draw keypoints\n",
    "    for i, (x, y) in enumerate(kxy):\n",
    "        if i < len(kconf) and kconf[i] >= vis_thr:\n",
    "            cv2.circle(frame, (int(x), int(y)), 5, (255,0,255), -1)\n",
    "\n",
    "    # draw skeleton lines\n",
    "    for a, b in skeleton:\n",
    "        if a < len(kxy) and b < len(kxy) and kconf[a] >= vis_thr and kconf[b] >= vis_thr:\n",
    "            ax, ay = int(kxy[a][0]), int(kxy[a][1])\n",
    "            bx, by = int(kxy[b][0]), int(kxy[b][1])\n",
    "            cv2.line(frame, (ax, ay), (bx, by), (255,0,0), 2)\n",
    "\n",
    "    # indices for knees/ankles/hips in COCO-17\n",
    "    idx = dict(LHIP=11, LKNE=13, LANK=15, RHIP=12, RKNE=14, RANK=16)\n",
    "\n",
    "    def get_pt(i):\n",
    "        if i < len(kxy) and i < len(kconf) and kconf[i] >= vis_thr:\n",
    "            return (int(kxy[i][0]), int(kxy[i][1]))\n",
    "        return None\n",
    "\n",
    "    l_hip, l_knee, l_ank = get_pt(idx[\"LHIP\"]), get_pt(idx[\"LKNE\"]), get_pt(idx[\"LANK\"])\n",
    "    r_hip, r_knee, r_ank = get_pt(idx[\"RHIP\"]), get_pt(idx[\"RKNE\"]), get_pt(idx[\"RANK\"])\n",
    "\n",
    "    avg_knee_angle = None\n",
    "    try:\n",
    "        angles = []\n",
    "        if l_hip and l_knee and l_ank:\n",
    "            angles.append(calculate_angle(l_hip, l_knee, l_ank))\n",
    "        if r_hip and r_knee and r_ank:\n",
    "            angles.append(calculate_angle(r_hip, r_knee, r_ank))\n",
    "\n",
    "        if angles:\n",
    "            avg_knee_angle = int(np.mean(angles))\n",
    "\n",
    "            # rep logic\n",
    "            if avg_knee_angle < squat_down_angle and not squat_down:\n",
    "                squat_down = True\n",
    "            if avg_knee_angle > squat_up_angle and squat_down:\n",
    "                rep_count += 1\n",
    "                squat_down = False\n",
    "\n",
    "            # feedback\n",
    "            if avg_knee_angle > squat_up_angle:\n",
    "                feedback_message = \"Too Upright\"\n",
    "            elif avg_knee_angle < 60:\n",
    "                feedback_message = \"Too Deep!\"\n",
    "            else:\n",
    "                feedback_message = \"Good Depth\"\n",
    "\n",
    "            # overlay\n",
    "            cv2.putText(frame, f\"Knee Angle: {avg_knee_angle} degrees\", (30, 80),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # log this frame; sway filled later by track_barbell\n",
    "    log_data.append({\n",
    "        \"frame\": frame_idx,\n",
    "        \"angle\": avg_knee_angle,\n",
    "        \"reps\": rep_count,\n",
    "        \"feedback\": feedback_message,\n",
    "        \"sway\": None\n",
    "    })\n",
    "\n",
    "    return frame, avg_knee_angle\n",
    "\n",
    "# =============================\n",
    "# 5) Barbell tracking\n",
    "# =============================\n",
    "\n",
    "def track_barbell(frame, results, frame_idx):\n",
    "    global feedback_message\n",
    "\n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        conf = float(box.conf[0])\n",
    "        if conf > 0.4:\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            barbell_path.append((cx, cy))\n",
    "            bar_center_history.append(cx)\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "            cv2.putText(frame, \"Barbell\", (x1, y1 - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "    for i in range(1, len(barbell_path)):\n",
    "        cv2.line(frame, barbell_path[i - 1], barbell_path[i], (0, 165, 255), 2)\n",
    "\n",
    "    if len(bar_center_history) > 10:\n",
    "        sway_range = max(bar_center_history[-10:]) - min(bar_center_history[-10:])\n",
    "        sway_values.append(sway_range)\n",
    "        if sway_range > max_sway:\n",
    "            feedback_message = \"Bar Sway Detected!\"\n",
    "\n",
    "        if len(log_data) > 0:\n",
    "            if log_data[-1].get(\"frame\", None) == frame_idx:\n",
    "                log_data[-1][\"sway\"] = sway_range\n",
    "\n",
    "    return frame\n",
    "\n",
    "# =============================\n",
    "# 6) Video I/O\n",
    "# =============================\n",
    "video_folder = \"Input_Video\"\n",
    "video_filename = \"Blue_Plates_BB_Squat_Dark Setting.mp4\"\n",
    "video_path = os.path.join(video_folder, video_filename)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Could not open video: {video_path}\")\n",
    "\n",
    "# main output folder\n",
    "output_folder = \"Output_Video\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# subfolder for CSV results\n",
    "results_folder = os.path.join(output_folder, \"Results\")\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "# video output file\n",
    "output_filename = \"Blue_Plates_BB_Squat_Dark Setting_output_pose_squat_system.mp4\"\n",
    "output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "# set up VideoWriter properly\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps_in = cap.get(cv2.CAP_PROP_FPS)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps_in, (w, h))\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 7) Main loop\n",
    "# =============================\n",
    "frame_idx = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # pose + log\n",
    "    pose_frame, angle = detect_pose(frame.copy(), frame_idx)\n",
    "\n",
    "    # barbell detect + sway + draw\n",
    "    results = yolo_model(frame, imgsz=416, conf=0.35, verbose=False)[0]\n",
    "    pose_frame = track_barbell(pose_frame, results, frame_idx)\n",
    "\n",
    "    cv2.putText(pose_frame, f\"Reps: {rep_count}\", (30, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    if feedback_message:\n",
    "        cv2.putText(pose_frame, feedback_message, (30, 120),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 3)\n",
    "\n",
    "    cv2.imshow(\"Squat Analyzer (YOLOv8-Pose)\", pose_frame)\n",
    "    out.write(pose_frame)\n",
    "\n",
    "    frame_idx += 1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "avg_fps = frame_idx / elapsed if elapsed > 0 else 0.0\n",
    "print(f\"Saved video: {output_path}\")\n",
    "print(f\"Average FPS: {avg_fps:.2f}\")\n",
    "\n",
    "# =============================\n",
    "# 8) Save logs\n",
    "# =============================\n",
    "df = pd.DataFrame(log_data)\n",
    "df[\"fps\"] = avg_fps\n",
    "\n",
    "# Per-frame CSV\n",
    "perframe_csv = os.path.join(results_folder,\n",
    "    \"Blue_Plates_BB_Squat_Dark Setting_results_yolopose_perframe.csv\")\n",
    "df.to_csv(perframe_csv, index=False)\n",
    "\n",
    "# Summary CSV\n",
    "summary = {\n",
    "    \"total_frames\": frame_idx,\n",
    "    \"total_reps\": rep_count,\n",
    "    \"avg_fps\": avg_fps,\n",
    "    \"mean_angle\": float(df[\"angle\"].dropna().mean()) if df[\"angle\"].notna().any() else None,\n",
    "    \"std_angle\": float(df[\"angle\"].dropna().std()) if df[\"angle\"].notna().any() else None,\n",
    "    \"mean_sway\": float(df[\"sway\"].dropna().mean()) if df[\"sway\"].notna().any() else None,\n",
    "    \"max_sway\": float(df[\"sway\"].dropna().max()) if df[\"sway\"].notna().any() else None\n",
    "}\n",
    "summary_csv = os.path.join(results_folder,\n",
    "    \"Blue_Plates_BB_Squat_Dark Setting_results_yolopose_summary.csv\")\n",
    "pd.DataFrame([summary]).to_csv(summary_csv, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
